{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from queue import Queue\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/\".join(os.getcwd().split(\"/\")[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.settings.general import FRAME_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.settings.trigger import TX, FX, N_CLASSES, TRIGGER_KERNEL_SIZE, TRIGGER_STRIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_model(input_shape, n_classes, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "\n",
    "    :param input_shape: shape of the model's input data (using Keras conventions)\n",
    "    :param n_classes: n_classes to predict for the last dense layer\n",
    "    :param kernel_size: kernel size of the first conv layer\n",
    "    :param stride : stride_size of the first conv layer\n",
    "    :return: Keras model instance\n",
    "    \"\"\"\n",
    "\n",
    "    X_input = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Step 1: CONV layer (≈4 lines)\n",
    "    X = tf.keras.layers.Conv1D(512, kernel_size=kernel_size, strides=stride)(X_input)  # CONV1D\n",
    "    X = tf.keras.layers.BatchNormalization()(X)  # Batch normalization\n",
    "    X = tf.keras.layers.Activation('relu')(X)  # ReLu activation\n",
    "    X = tf.keras.layers.Dropout(0.2)(X)  # dropout (use 0.8)\n",
    "\n",
    "    # Step 2: First GRU Layer (≈4 lines)\n",
    "    X = tf.keras.layers.GRU(units=256, return_sequences=True)(X)  # GRU (use 128 units and return the sequences)\n",
    "    X = tf.keras.layers.Dropout(0.2)(X)  # dropout (use 0.8)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)  # Batch normalization\n",
    "\n",
    "    # Step 3: Second GRU Layer (≈4 lines)\n",
    "    X = tf.keras.layers.GRU(units=256, return_sequences=True)(X)  # GRU (use 128 units and return the sequences)\n",
    "    X = tf.keras.layers.Dropout(0.2)(X)  # dropout (use 0.8)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)  # Batch normalization\n",
    "\n",
    "    # Step 4: Time-distributed dense layer (≈1 line)\n",
    "    X = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_classes, activation=\"softmax\"))(X)  # time distributed (sigmoid)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=X_input, outputs=X)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0814 11:30:58.582641 140734894839232 deprecation.py:506] From /Users/az01640/Projets/multrigger-word/.venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 860, 257)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 212, 512)          1974272   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 212, 512)          2048      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 212, 512)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 212, 512)          0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 212, 256)          590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 212, 256)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 212, 256)          1024      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 212, 256)          393984    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 212, 256)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 212, 256)          1024      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 212, 2)            514       \n",
      "=================================================================\n",
      "Total params: 2,963,458\n",
      "Trainable params: 2,961,410\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x14199ecf8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(\"../logs/trigger/checkpoints/\")\n",
    "\n",
    "model = trigger_model(input_shape=(TX, FX),\n",
    "                      n_classes=N_CLASSES,\n",
    "                      kernel_size=TRIGGER_KERNEL_SIZE,\n",
    "                      stride=TRIGGER_STRIDE)\n",
    "\n",
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_input_stream(callback):\n",
    "    stream = pyaudio.PyAudio().open(\n",
    "        format=pyaudio.paInt16,\n",
    "        channels=1,\n",
    "        rate=FRAME_RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=CHUNK_SAMPLES,\n",
    "        input_device_index=0,\n",
    "        stream_callback=callback)\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.audio import load_raw_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives, backgrounds = load_raw_audio()\n",
    "positive_labels = sorted(positives.keys())\n",
    "\n",
    "MAP_DICT = dict(enumerate(positive_labels, 1))\n",
    "MAP_DICT[0] = \"background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_triggerword_spectrum(model, x):\n",
    "    \"\"\"\n",
    "    Function to predict the location of the trigger word.\n",
    "\n",
    "    :param model: neural network that is use for inference\n",
    "    :param x: spectrum of shape (TX, FX)\n",
    "    :return: predictions -- numpy ndarray to shape (number of output time steps per num_classes)\n",
    "    \"\"\"\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    predictions = model.predict(x)[0]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_new_triggerword(predictions, chunk_duration, feed_duration, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Function to detect new trigger word in the latest chunk of input audio.\n",
    "    It is looking for the rising edge of the predictions data belongs to the\n",
    "    last/latest chunk.\n",
    "\n",
    "    :param predictions:  predicted labels from model\n",
    "    :param chunk_duration: time in second of a chunk\n",
    "    :param feed_duration: time in second of the input to model\n",
    "    :param threshold: threshold for probability above a certain to be considered positive\n",
    "    :return: True if new trigger word detected in the latest chunk\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = predictions[:,1] > threshold\n",
    "    chunk_predictions_samples = int(len(predictions) * chunk_duration / feed_duration)\n",
    "    chunk_predictions = predictions[-chunk_predictions_samples:]\n",
    "    level = chunk_predictions[0]\n",
    "    for pred in chunk_predictions:\n",
    "        if pred > level:\n",
    "            return True\n",
    "        else:\n",
    "            level = pred\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.audio import get_spectrogram\n",
    "from src.trigger.make_dataset import transform_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".27.761600017547607\n",
      ".24.97511386871338\n",
      ".22.745832204818726\n",
      ".19.95950198173523\n",
      "--.12.715651988983154\n",
      ".9.928544998168945\n",
      ".7.699174165725708\n",
      "--.-0.00853109359741211\n",
      "---------------"
     ]
    }
   ],
   "source": [
    "# Queue to communiate between the audio callback and main thread\n",
    "q = Queue()\n",
    "\n",
    "run = True\n",
    "\n",
    "silence_threshold = 100\n",
    "\n",
    "CHUNK_DURATION = 5 # Each read length in seconds from mic.\n",
    "CHUNK_SAMPLES = int(FRAME_RATE * CHUNK_DURATION) # Each read length in number of samples.FEED_SAMPLES = int(FRAME_RATE * FEED_DURATION)\n",
    "\n",
    "FEED_DURATION = .5 # Each read length in seconds from mic.\n",
    "FEED_SAMPLES = int(FRAME_RATE * FEED_DURATION)\n",
    "\n",
    "# Run the demo for a timeout seconds\n",
    "timeout = time.time() + 0.5*60  # 0.5 minutes from now\n",
    "\n",
    "# Data buffer for the input wavform\n",
    "data = np.zeros(CHUNK_SAMPLES, dtype='int16')\n",
    "\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    global run, timeout, data, silence_threshold    \n",
    "    if time.time() > timeout:\n",
    "        run = False        \n",
    "    data0 = np.frombuffer(in_data, dtype='int16')\n",
    "    if np.abs(data0).mean() < silence_threshold:\n",
    "        sys.stdout.write('-')\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "    else:\n",
    "        sys.stdout.write('.')\n",
    "    data = np.append(data,data0)    \n",
    "    if len(data) > CHUNK_SAMPLES:\n",
    "        data = data[-CHUNK_SAMPLES:]\n",
    "        # Process data async by sending a queue.\n",
    "        q.put(data)\n",
    "    return (in_data, pyaudio.paContinue)\n",
    "\n",
    "stream = get_audio_input_stream(callback)\n",
    "stream.start_stream()\n",
    "\n",
    "try:\n",
    "    while run:\n",
    "        data = q.get()\n",
    "        print(timeout - time.time())\n",
    "        spectrum = get_spectrogram(data)\n",
    "        preds = detect_triggerword_spectrum(model, np.swapaxes(spectrum, 0,1))\n",
    "        new_trigger = has_new_triggerword(preds, CHUNK_DURATION, FEED_DURATION)\n",
    "        if new_trigger:\n",
    "            sys.stdout.write('1')\n",
    "except (KeyboardInterrupt, SystemExit):\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    timeout = time.time()\n",
    "    run = False\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.stop_stream()\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
