{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_SIZE = 15\n",
    "STRIDE = 4\n",
    "FRAME_RATE = 48000\n",
    "NFFT = 512\n",
    "TX = FRAME_RATE * 0.0195\n",
    "FX = int(NFFT / 2) + 1\n",
    "TY = round((TX - KERNEL_SIZE + STRIDE) / STRIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_feature(record, feature):\n",
    "    example = tf.train.Example.FromString(record.numpy())\n",
    "    return example.features.feature[feature].float_list.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tf record dataset\n",
    "def parser(record):\n",
    "    \n",
    "    X = tf.reshape(\n",
    "            tf.py_function(\n",
    "            lambda r: _extract_feature(r, \"X\"),\n",
    "            (record,),\n",
    "            tf.float32\n",
    "        ), [Tx, n_freq]\n",
    "    )\n",
    "    \n",
    "    Y = tf.reshape(\n",
    "        tf.py_function(\n",
    "            lambda r: _extract_feature(r, \"Y\"),\n",
    "            (record,),\n",
    "            tf.float32\n",
    "        ), [Ty, num_classes]\n",
    "    )\n",
    "    \n",
    "    return X, Y\n",
    "    \n",
    "def dataset_input_fn(filenames, batch_size, num_epochs):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    #iterator = dataset.make_one_shot_iterator()\n",
    "    #features, labels = iterator.get_next()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from tensorflow.keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_model(input_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    \n",
    "    Argument:\n",
    "    input_shape -- shape of the model's input data (using Keras conventions)\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input = Input(shape = input_shape)\n",
    "        \n",
    "    # Step 1: CONV layer (≈4 lines)\n",
    "    X = Conv1D(196, kernel_size=KERNEL_SIZE, strides=STRIDE)(X_input)                                 # CONV1D\n",
    "    X = BatchNormalization()(X)                                 # Batch normalization\n",
    "    X = Activation('relu')(X)                                 # ReLu activation\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "\n",
    "    # Step 2: First GRU Layer (≈4 lines)\n",
    "    X = GRU(units = 128, return_sequences = True)(X) # GRU (use 128 units and return the sequences)\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "    X = BatchNormalization()(X)                                 # Batch normalization\n",
    "    \n",
    "    # Step 3: Second GRU Layer (≈4 lines)\n",
    "    X = GRU(units = 128, return_sequences = True)(X)   # GRU (use 128 units and return the sequences)\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "    X = BatchNormalization()(X)                                  # Batch normalization\n",
    "    X = Dropout(0.8)(X)                                  # dropout (use 0.8)\n",
    "    \n",
    "    # Step 4: Time-distributed dense layer (≈1 line)\n",
    "    X = TimeDistributed(Dense(n_classes, activation = \"sigmoid\"))(X) # time distributed  (sigmoid)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = seq_model((TX, FX), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 936, 257)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 231, 196)          755776    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 231, 196)          784       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 231, 196)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 231, 196)          0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 231, 128)          124800    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 231, 128)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 231, 128)          512       \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 231, 128)          98688     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 231, 128)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 231, 128)          512       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 231, 128)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 231, 1)            129       \n",
      "=================================================================\n",
      "Total params: 981,201\n",
      "Trainable params: 980,297\n",
      "Non-trainable params: 904\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "keras_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 11s 1s/step - loss: 1.3719 - acc: 0.4976\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 9s 931ms/step - loss: 1.3102 - acc: 0.5057\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 9s 933ms/step - loss: 1.2790 - acc: 0.5100\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 13s 1s/step - loss: 1.2967 - acc: 0.4984\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 1.2548 - acc: 0.5049\n"
     ]
    }
   ],
   "source": [
    "training_set = dataset_input_fn(tfrecord_path, 16, None)\n",
    "\n",
    "history = keras_model.fit(\n",
    "    training_set.make_one_shot_iterator(),\n",
    "    steps_per_epoch=10,\n",
    "    epochs=5,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
